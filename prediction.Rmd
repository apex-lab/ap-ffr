---
title: "Absolute Pitch Recognition and the Frequency Following Response"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) 
set.seed(1)
theme_set(theme_minimal())
```

# Comparing the Predictive Power of the FFRs to the Three Stimuli

We fit a Lasso regression model on each stimulus (with the regularization parameter $\lambda$ chosen to minimize $k$-folds cross validation loss on the training set), and then validate the model on a test set. This procedure is repeated many times, calculating $r = \text{corr}(\hat{y}, y)$ on each repetition. The $r$ values are then compared to chance performance $H_0: r = 0$ and to $r$ from models trained on the other stimuli.

```{r}
library(glmnet)
library(car)
library(ggplot2)


empLogit <- function(x, eps = 1e-3) log((eps + x)/(1 - x+eps))
invLogit <- function(l, eps = 1e-3) (exp(l) + eps*exp(l) - eps)/(1 + exp(l))

cross_validation <- function(x, y) { # performs one trial of cross validation procedure
  # logit transform y so accuracy isn't [0, 1] bounded
  y <- empLogit(y)
  mu <- mean(y)
  s <- sd(y)
  y <- (y - mu)/s
  # randomly split into training and test set
  train <- sample(1:nrow(x), nrow(x)/3) 
  test <- -train
  # grid search for regularization param, testing values on folds of training data
  grid <- 10^seq(10, -2, length = 100) 
  lasso.fit <- glmnet(x[train,], y[train], alpha = 1, lambda = grid) 
  n <- length(y[train])
  cv.out <- cv.glmnet(x[train,], y[train], alpha = 1, nfolds = n)
  # test model with optimal param on hold-out data
  predictions <- predict(lasso.fit, s = cv.out$lambda.min, newx = x[test,]) 
  predictions <- invLogit(predictions*s + mu)
  y_test <- invLogit(y[test]*s + mu)
  r <- cor(predictions, y_test) # return correlation b/w predictions and test vals
  return(r)
}

get_coeffs <- function(x, y) {
  y <- empLogit(y)
  mu <- mean(y)
  s <- sd(y)
  y <- (y - mu)/s
  grid <- 10^seq(10, -2, length = 100) 
  lasso.fit <- glmnet(x, y, alpha = 1, lambda = grid)
  cv.out <- cv.glmnet(x, y, alpha = 1, nfolds = length(y))
  lasso.coef <- predict(lasso.fit, type = "coefficients", s = cv.out$lambda.min)
  return(lasso.coef)
}

get_preds <- function(x, y, groups) {
  y <- empLogit(y)
  mu <- mean(y)
  s <- sd(y)
  y <- (y - mu)/s
  grid <- 10^seq(10, -2, length = 100) 
  lasso.fit <- glmnet(x, y, alpha = 1, lambda = grid)
  cv.out <- cv.glmnet(x, y, alpha = 1, nfolds = length(y))
  predictions <- predict(lasso.fit, s = cv.out$lambda.min, newx = x) 
  yhat <- invLogit(predictions*s + mu)
  y <- invLogit(y*s + mu)
  df <- data.frame(y, yhat, groups)
  colnames(df) <- c('observed', 'predicted', 'groups')
  p <- ggplot(df, aes(x=observed, y=predicted, color = groups)) + geom_point() + 
    geom_abline(intercept = 0, slope = 1, linetype="dashed") + 
    xlim(0, 1) + ylim(0, 1) + coord_fixed() + theme(legend.position = "none")
  return(p)
}

```

```{r}
# load data
x_piano <- read.csv("data/harmonics/piano_harmonics.csv", header = FALSE) # bad subjects already removed
colnames(x_piano) <- paste('H', 0:(ncol(x_piano) - 1), sep = "")
x_complex <- read.csv("data/harmonics/complex_harmonics.csv", header = FALSE)
colnames(x_complex) <- paste('H', 0:(ncol(x_complex) - 1), sep = "")
x_da <- read.csv("data/harmonics/da_harmonics.csv", header = FALSE)
colnames(x_da) <- paste('H', 0:(ncol(x_da) - 1), sep = "")
behav <- read.csv("data/AP_FFR_Behavioral.csv") # bad subjects already removed


# remove subjects with no pitch scores
x_piano <- x_piano[!is.na(behav$AP_Piano_ACC) & !is.na(behav$Pitch.Adjustment) & behav$Group != 'NO',]
x_complex <- x_complex[!is.na(behav$AP_Piano_ACC) & !is.na(behav$Pitch.Adjustment) & behav$Group != 'NO',]
x_da <- x_da[!is.na(behav$AP_Piano_ACC) & !is.na(behav$Pitch.Adjustment) & behav$Group != 'NO',]
behav <- behav[!is.na(behav$AP_Piano_ACC) & !is.na(behav$Pitch.Adjustment) & behav$Group != 'NO',]


# put things in order for glmnet (must be in matrix format)
x_piano <- scale(as.matrix(x_piano))
x_complex <- scale(as.matrix(x_complex))
x_da <- scale(as.matrix(x_da))
y_piano <- behav$AP_Piano_Conservative
y_sine <- behav$AP_Sine_Conservative
y_avg <- (36*y_piano + 40*y_sine)/(36+40)
#y_behav <- behav["AgeOnset_instrument","JND", "Pitch.Adjustment"]
jnd <- behav$JND
pitchAdjustment <- behav$Pitch.Adjustment
tonal <- behav$Tonal
ageOnset <- behav$AgeOnset_instrument
x_behav <- data.frame(jnd, pitchAdjustment, ageOnset)
x_behav <- scale(as.matrix(x_behav)) # normalize everything but tonal 
x_behav <- cbind(x_behav, tonal) # then add in tonal dummy variable
```

```{r, warning = FALSE}
iters <- 1000 # number of times to perform cross validation
r_piano <- 1:iters # placeholder array
r_complex <- 1:iters
r_da <- 1:iters
for (iter in 1:iters) {
  r_piano[iter] <- cross_validation(x_piano, y_avg)
  r_complex[iter] <- cross_validation(x_complex, y_avg)
  r_da[iter] <- cross_validation(x_da, y_avg)
}

# compile all r values into one dataframe for plotting
r_piano <- data.frame(r_piano)
colnames(r_piano) <- c("r")
r_piano$stimulus <- "piano tone"
r_complex <- data.frame(r_complex)
colnames(r_complex) <- c("r")
r_complex$stimulus <- "complex tone"
r_da <- data.frame(r_da)
colnames(r_da) <- c("r")
r_da$stimulus <- "speech: /da/"
r_vals <- rbind(r_piano, r_complex, r_da)

# apply Fisher z-transformation to correlation values
r_vals$z <- (log(1 + r_vals$r) - log(1 - r_vals$r)) / 2
```

```{r}
library(ggplot2)
library(ggpubr)
library(gridExtra)
library(cowplot)
p1 <- ggplot(r_vals, aes(x = stimulus, y = r, fill = stimulus)) +
  geom_violin() + stat_summary(fun = mean, geom = "point") +
  theme_minimal() + guides(fill = FALSE) + 
  labs(x = "Stimulus", y = "Correlation Values") #+ ylim(c(-1.2, 1.2))
comparisons = list(c(1, 2), c(2, 3), c(1, 3))
p2 <- ggplot(r_vals, aes(x = stimulus, y = z, fill = stimulus)) +
  geom_violin() + stat_summary(fun = mean, geom = "point") +
  stat_compare_means(method = "t.test", label =  "p.signif", comparisons=comparisons, paired = F) +
  theme_minimal() + guides(fill = FALSE) + 
  labs(x = "Stimulus", y = "Z-transformed Correlations") + ylim(c(-.8, 1.8))
p3 <- get_preds(x_complex, y_avg, behav$Group)
p4 <- get_preds(x_piano, y_avg, behav$Group)
p5 <- get_preds(x_da, y_avg, behav$Group)
g1 <- ggarrange(p1, p2,
                ncol = 2, nrow = 1, labels = c("A", "B"), label.x = .9)
g2 <- ggarrange(p3, p4, p5,
                ncol = 3, nrow = 1,
                labels = c("C", "D", "E"), label.x = .93)
ggarrange(g1, g2, ncol = 1, nrow = 2)
```

```{r}
get_coeffs(x_complex, y_avg)
get_coeffs(x_piano, y_avg) 
get_coeffs(x_da, y_avg)
```

```{r}
# compute t-stats to spit out in-line
p_vs_z <- t.test(r_vals$z[r_vals$stimulus == "piano tone"])
c_vs_z <- t.test(r_vals$z[r_vals$stimulus == "complex tone"])
d_vs_z <- t.test(r_vals$z[r_vals$stimulus == "speech: /da/"])
p_vs_c <- t.test(r_vals$z[r_vals$stimulus == "piano tone"], r_vals$z[r_vals$stimulus == "complex tone"])
p_vs_d <- t.test(r_vals$z[r_vals$stimulus == "piano tone"], r_vals$z[r_vals$stimulus == "speech: /da/"])
c_vs_d <- t.test(r_vals$z[r_vals$stimulus == "complex tone"], r_vals$z[r_vals$stimulus == "speech: /da/"])
pm <- mean(r_vals$r[r_vals$stimulus == "piano tone"])
cm <- mean(r_vals$r[r_vals$stimulus == "complex tone"])
dm <- mean(r_vals$r[r_vals$stimulus == "speech: /da/"])
```


The FFR to the piano tone ($r$ = `r round(pm, 2)`, $t$(`r p_vs_z$parameter`) = `r round(p_vs_z$statistic, 2)`, $p$ = `r formatC(p_vs_z$p.value, format = "e", digits = 2)`) and the FFR to the unfamiliar complex tone ($r$ = `r round(cm, 2)`, $t$(`r c_vs_z$parameter`) = `r round(c_vs_z$statistic, 2)`, $p$ = `r formatC(c_vs_z$p.value, format = "e", digits = 2)`) both predict pitch-labelling performance better than chance, but not significantly differently from one another ($t$(`r round(p_vs_c$parameter, 2)`) = `r round(p_vs_c$statistic, 2)`, $p$ = `r formatC(p_vs_c$p.value, format = "e", digits = 2)`). Both the piano tone FFR ($t$(`r round(p_vs_d$parameter,2)`) = `r round(p_vs_d$statistic, 2)`, $p$ = `r formatC(p_vs_d$p.value, format = "e", digits = 2)`) and complex tone FFR ($t$(`r round(c_vs_d$parameter, 2)`) = `r round(c_vs_d$statistic, 2)`) perform significantly better than the speech-evoked FFR ($r$ = `r round(dm, 2)`), which performs significantly worse than chance ($t$(`r d_vs_z$parameter`) = `r round(d_vs_z$statistic, 2)`, $p$ = `r formatC(d_vs_z$p.value, format = "e", digits = 2)`).


# Does Piano Tone predict AP performance for piano tones more than for sine tones?

```{r, warning = FALSE}
r_pianoacc <- 1:iters # placeholder array
r_sineacc <- 1:iters
for (iter in 1:iters) {
  r_pianoacc[iter] <- cross_validation(x_piano, y_piano)
  r_sineacc[iter] <- cross_validation(x_piano, y_sine)
}

# compile all r values into one dataframe for plotting
r_pianoacc <- data.frame(r_pianoacc)
colnames(r_pianoacc) <- c("r")
r_pianoacc$stimulus <- "piano tones"
r_sineacc <- data.frame(r_sineacc)
colnames(r_sineacc) <- c("r")
r_sineacc$stimulus <- "sine tones"
r_vals_type <- rbind(r_pianoacc, r_sineacc)


# apply Fisher z-transformation to correlation values
r_vals_type$z <- (log(1 + r_vals_type$r) - log(1 - r_vals_type$r)) / 2
```

```{r}
p1 <- ggplot(r_vals_type, aes(x = stimulus, y = r, fill = stimulus)) +
  geom_violin() + stat_summary(fun = mean, geom = "point") +
  theme_minimal() + guides(fill = FALSE) + 
  labs(x = "Test", y = "Correlation Values")
comparisons = list(c(1, 2))
p2 <- ggplot(r_vals_type, aes(x = stimulus, y = z, fill = stimulus)) +
  geom_violin() + stat_summary(fun = mean, geom = "point") +
  stat_compare_means(method = "t.test", label =  "p.signif", comparisons=comparisons, paired = F) +
  theme_minimal() + guides(fill = FALSE) + 
  labs(x = "Test", y = "Z-transformed Correlations") + ylim(c(-.8, 1.75))
p3 <- get_preds(x_piano, y_piano, behav$Group)
p4 <- get_preds(x_piano, y_sine, behav$Group)
g1 <- ggarrange(p1, p2,
                ncol = 2, nrow = 1, labels = c("A", "B"), label.x = .9)
g2 <- ggarrange(p3, p4,
                ncol = 2, nrow = 1,
                labels = c("C", "D"), label.x = .93)
ggarrange(g1, g2, ncol = 1, nrow = 2)
```

```{r}
get_coeffs(x_piano, y_piano)
get_coeffs(x_piano, y_sine)
```

```{r}
yp_vs_z <- t.test(r_vals_type$z[r_vals_type$stimulus == "piano tones"])
ys_vs_z <- t.test(r_vals_type$z[r_vals_type$stimulus == "sine tones"])
yp_vs_ys <- t.test(r_vals_type$z[r_vals_type$stimulus == "piano tones"], 
                   r_vals_type$z[r_vals_type$stimulus == "sine tones"])
ypm <- mean(r_vals_type$r[r_vals_type$stimulus == "piano tones"])
ysm <- mean(r_vals_type$r[r_vals_type$stimulus == "sine tones"])
```

The piano tone FFR predicts subject performance both on piano tones ($r$ = `r round(ypm, 2)`, $t$(`r yp_vs_z$parameter`) = `r round(yp_vs_z$statistic, 2)`, $p$ = `r formatC(yp_vs_z$p.value, format = "e", digits = 2)`) and on sine tones ($r$ = `r round(ysm, 2)`, $t$(`r ys_vs_z$parameter`) = `r round(ys_vs_z$statistic, 2)`, $p$ = `r formatC(ys_vs_z$p.value, format = "e", digits = 2)`). However, the model does predict significantly better on piano tone performance ($t$(`r round(yp_vs_ys$parameter, 2)`) = `r round(yp_vs_ys$statistic, 2)`, $p$ = `r formatC(yp_vs_ys$p.value, format = "e", digits = 2)`), suggesting a more specific effect of auditory encoding on pitch classification ability. 


# EEG vs. Behavioral Data
```{r, warning = FALSE}

x_both <- cbind(x_piano, x_behav)

r_piano <- 1:iters # placeholder array
r_behav <- 1:iters
r_both <- 1:iters
for (iter in 1:iters) {
  r_piano[iter] <- cross_validation(x_piano, y_avg)
  r_behav[iter] <- cross_validation(x_behav, y_avg)
  r_both[iter] <- cross_validation(x_both, y_avg)
}

# compile all r values into one dataframe for plotting
r_piano <- data.frame(r_piano)
colnames(r_piano) <- c("r")
r_piano$stimulus <- "FFR"
r_behav <- data.frame(r_behav)
colnames(r_behav) <- c("r")
r_behav$stimulus <- "behavioral"
r_both <- data.frame(r_both)
colnames(r_both) <- c("r")
r_both$stimulus <- "both"
r_vals <- rbind(r_piano, r_behav, r_both)

# apply Fisher z-transformation to correlation values
r_vals$z <- (log(1 + r_vals$r) - log(1 - r_vals$r)) / 2
```

```{r}
p1 <- ggplot(r_vals, aes(x = stimulus, y = r, fill = stimulus)) +
  geom_violin() + stat_summary(fun = mean, geom = "point") +
  theme_minimal() + guides(fill = FALSE) + 
  labs(x = "Predictors", y = "Correlation Values") #+ ylim(c(-1.2, 1.2))
comparisons = list(c(1, 2), c(2, 3), c(1, 3))
p2 <- ggplot(r_vals, aes(x = stimulus, y = z, fill = stimulus)) +
  geom_violin() + stat_summary(fun = mean, geom = "point") +
  stat_compare_means(method = "t.test", label =  "p.signif", comparisons=comparisons, paired = F) +
  theme_minimal() + guides(fill = FALSE) + 
  labs(x = "Predictors", y = "Z-transformed Correlations") + ylim(c(-1.2, 1.5))
p3 <- get_preds(x_behav, y_avg, behav$Group)
p4 <- get_preds(x_both, y_avg, behav$Group)
p5 <- get_preds(x_piano, y_avg, behav$Group)
g1 <- ggarrange(p1, p2,
                ncol = 2, nrow = 1, labels = c("A", "B"), label.x = .9)
g2 <- ggarrange(p3, p4, p5,
                ncol = 3, nrow = 1,
                labels = c("C", "D", "E"), label.x = .93)
ggarrange(g1, g2, ncol = 1, nrow = 2)
```

```{r}
get_coeffs(x_behav, y_avg)
get_coeffs(x_both, y_avg) 
get_coeffs(x_piano, y_avg)
```

```{r}
# compute t-stats to spit out in-line
beh_vs_z <- t.test(r_vals$z[r_vals$stimulus == "behavioral"])
both_vs_z <- t.test(r_vals$z[r_vals$stimulus == "both"])
beh_vs_both <- t.test(r_vals$z[r_vals$stimulus == "behavioral"], r_vals$z[r_vals$stimulus == "both"])
both_vs_ffr <- t.test(r_vals$z[r_vals$stimulus == "both"], r_vals$z[r_vals$stimulus == "FFR"])
beh_vs_ffr <- t.test(r_vals$z[r_vals$stimulus == "behavioral"], r_vals$z[r_vals$stimulus == "FFR"])
behm <- mean(r_vals$r[r_vals$stimulus == "behavioral"])
bothm <- mean(r_vals$r[r_vals$stimulus == "both"])
ffrm <- mean(r_vals$r[r_vals$stimulus == "FFR"])
```

The frequency following responses to the piano tone predicts AP performance better than the behavioral measures (age of music onset, tonal language experience, pitch adjustment and just-noticeable-difference scores) are able to  ($t$(`r beh_vs_ffr$parameter`) = `r round(beh_vs_ffr$statistic, 2)`, $p$ = `r formatC(beh_vs_ffr$p.value, format = "e", digits = 2)`), with the latter only performing slightly, albeit significantly, above chance ($r$ = `r round(behm, 2)`, $t$(`r beh_vs_z$parameter`) = `r round(beh_vs_z$statistic, 2)`, $p$ = `r formatC(beh_vs_z$p.value, format = "e", digits = 2)`). Notably, combining the behavioral and electrophysiological predictors ($r$ = `r round(bothm, 2)`) yields a model that is worse than that based on only electrophysiological predictors ($t$(`r both_vs_ffr$parameter`) = `r round(both_vs_ffr$statistic, 2)`, $p$ = `r formatC(both_vs_ffr$p.value, format = "e", digits = 2)`), but does do better than the behavioral data alone ($t$(`r beh_vs_both$parameter`) = `r round(beh_vs_both$statistic, 2)`, $p$ = `r formatC(beh_vs_both$p.value, format = "e", digits = 2)`). This suggests that the behavioral measures contain little information about pitch labelling ability that is not already captured by the FFR. Interestingly, the behavioral-only model (see Equation X) removed all predictors except for the just-noticeable-difference score, a measure of perceptual discrimination ability, indicating that the other behavioral measures do not provide additional information about pitch labelling ability.


```{r}
sessionInfo() # a meager attempt at "package management" in a conda-less world
```




